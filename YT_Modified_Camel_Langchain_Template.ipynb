{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamescyl/myrepo/blob/main/YT_Modified_Camel_Langchain_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRYSu48huSUW",
        "outputId": "3570fe47-d6d6-432b-b323-483d5e5861df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.8/146.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.5/664.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#!pip -q install langchain openai tiktoken\n",
        "!pip -q install langchain\n",
        "!pip -q install google-generativeai langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Camel Dual Agent Interactions"
      ],
      "metadata": {
        "id": "Iiw2yKU_4P3f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "outputs": [],
      "source": [
        "#import os\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-KFB7J_u_3L"
      },
      "outputs": [],
      "source": [
        "!pip show langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      },
      "source": [
        "### Setting up OpenAI - gpt-3.5-turbo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zc4OIwSqI-Md"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "#from langchain.callbacks import get_openai_callback\n",
        "#from langchain.chat_models import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts.chat import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    BaseMessage,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GQYtqZhsJBGc"
      },
      "outputs": [],
      "source": [
        "class CAMELAgent:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        system_message: SystemMessage,\n",
        "#        model: ChatOpenAI,\n",
        "        model: ChatGoogleGenerativeAI,\n",
        "    ) -> None:\n",
        "        self.system_message = system_message\n",
        "        self.model = model\n",
        "        self.init_messages()\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        self.init_messages()\n",
        "        return self.stored_messages\n",
        "\n",
        "    def init_messages(self) -> None:\n",
        "        self.stored_messages = [self.system_message]\n",
        "\n",
        "    def update_messages(self, message: BaseMessage) -> List[BaseMessage]:\n",
        "        self.stored_messages.append(message)\n",
        "        #print(self.stored_messages)\n",
        "        return self.stored_messages\n",
        "\n",
        "    def step(\n",
        "        self,\n",
        "        input_message: HumanMessage,\n",
        "    ) -> AIMessage:\n",
        "        messages = self.update_messages(input_message)\n",
        "\n",
        "#        output_message = self.model(messages)\n",
        "        print(\"\\ninvoke\\n\", messages)\n",
        "        output_message = self.model.invoke(messages)\n",
        "        self.update_messages(output_message)\n",
        "\n",
        "        return output_message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjEESxmdJR8u"
      },
      "source": [
        "## Setup - The High Level task and roles for role-playing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6G0O5dbAJMFm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# assistant_role_name = \"Food lover 1\"\n",
        "# user_role_name = \"Food lover 2\"\n",
        "# task = \"Discuss the best meals they have ever eaten\"\n",
        "\n",
        "assistant_role_name = \"Singapore Tourism Board \"\n",
        "user_role_name = \"Tourist that has never been to Singapore\"\n",
        "task = \"Discuss the best tourist attractions to see in Singapore\"\n",
        "\n",
        "word_limit = 50 # word limit for task brainstorming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbGyMRf9JfsV"
      },
      "source": [
        "## Use Inception prompting to get a better task prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJHUCm1pJe_V",
        "outputId": "d726052d-7306-4755-8ce9-7ede7b8936ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Specified task: Unveil the vibrant tapestry of Singapore's iconic landmarks and hidden gems. Embark on an extraordinary journey through the Gardens by the Bay's futuristic wonders, marvel at the awe-inspiring Supertrees, and immerse yourself in the captivating story of the National Museum.\n"
          ]
        }
      ],
      "source": [
        "task_specifier_sys_msg = SystemMessage(content=\"You can make a task more specific.\")\n",
        "task_specifier_prompt = (\n",
        "\"\"\"Here is a task that {assistant_role_name} will discuss with {user_role_name} to : {task}.\n",
        "Please make it more specific. Be creative and imaginative.\n",
        "Please reply with the full task in {word_limit} words or less. Do not add anything else.\"\"\"\n",
        ")\n",
        "\n",
        "task_specifier_template = HumanMessagePromptTemplate.from_template(template=task_specifier_prompt)\n",
        "#task_specify_agent = CAMELAgent(task_specifier_sys_msg, ChatOpenAI(temperature=0.7))\n",
        "task_specify_agent = CAMELAgent(task_specifier_sys_msg, ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GEMINI_API_KEY))\n",
        "task_specifier_msg = task_specifier_template.format_messages(assistant_role_name=assistant_role_name,\n",
        "                                                             user_role_name=user_role_name,\n",
        "                                                             task=task, word_limit=word_limit)[0]\n",
        "specified_task_msg = task_specify_agent.step(task_specifier_msg)\n",
        "print(f\"Specified task: {specified_task_msg.content}\")\n",
        "specified_task = specified_task_msg.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjQ1BTX5JqwP"
      },
      "source": [
        "## Create inception prompts for AI assistant and AI user for role-playing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y-CeR0-5Jrh_"
      },
      "outputs": [],
      "source": [
        "assistant_inception_prompt = (\n",
        "\"\"\"Never forget you are a {assistant_role_name} and I am a {user_role_name}. Never flip roles!\n",
        "We share a common interest in collaborating to successfully complete a task.\n",
        "You must help me to complete the task.\n",
        "Here is the task: {task}. Never forget our task!\n",
        "I will instruct you based on your expertise and my needs to complete the task.\n",
        "\n",
        "I must give you one question at a time.\n",
        "You must write a specific answer that appropriately completes the requested question.\n",
        "You must decline my question honestly if you cannot comply the question due to physical, moral, legal reasons or your capability and explain the reasons.\n",
        "Do not add anything else other than your answer to my instruction.\n",
        "\n",
        "Unless I say the task is completed, you should always start with:\n",
        "\n",
        "My response: <YOUR_SOLUTION>\n",
        "\n",
        "<YOUR_SOLUTION> should be specific and descriptive.\n",
        "Always end <YOUR_SOLUTION> with: Next question.\"\"\"\n",
        ")\n",
        "\n",
        "user_inception_prompt = (\n",
        "\"\"\"Never forget you are a {user_role_name} and I am a {assistant_role_name}. Never flip roles! You will always ask me.\n",
        "We share a common interest in collaborating to successfully complete a task.\n",
        "I must help you to answer the questions.\n",
        "Here is the task: {task}. Never forget our task!\n",
        "You must instruct me based on my expertise and your needs to complete the task ONLY in the following two ways:\n",
        "\n",
        "1. Instruct with a necessary input:\n",
        "Instruction: <YOUR_INSTRUCTION>\n",
        "Input: <YOUR_INPUT>\n",
        "\n",
        "2. Instruct without any input:\n",
        "Instruction: <YOUR_INSTRUCTION>\n",
        "Input: None\n",
        "\n",
        "The \"Instruction\" describes a task or question. The paired \"Input\" provides further context or information for the requested \"Instruction\".\n",
        "\n",
        "You must give me one instruction at a time.\n",
        "I must write a response that appropriately completes the requested instruction.\n",
        "I must decline your instruction honestly if I cannot perform the instruction due to physical, moral, legal reasons or my capability and explain the reasons.\n",
        "You should instruct me not ask me questions.\n",
        "Now you must start to instruct me using the two ways described above.\n",
        "Do not add anything else other than your instruction and the optional corresponding input!\n",
        "Keep giving me instructions and necessary inputs until you think the task is completed.\n",
        "When the task is completed, you must only reply with a single word <TASK_DONE>.\n",
        "Never say <TASK_DONE> unless my responses have solved your task.\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoDdXh69JxVx"
      },
      "source": [
        "## Create a helper helper to get system messages for AI assistant and AI user from role names and the task\n",
        "\n",
        "- SystemMessage - the guidance\n",
        "- HumanMessage - input\n",
        "- AIMessage - the agent output/response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NAmLAfHzJyH4"
      },
      "outputs": [],
      "source": [
        "def get_sys_msgs(assistant_role_name: str, user_role_name: str, task: str):\n",
        "\n",
        "    assistant_sys_template = SystemMessagePromptTemplate.from_template(template=assistant_inception_prompt)\n",
        "    assistant_sys_msg = assistant_sys_template.format_messages(assistant_role_name=assistant_role_name, user_role_name=user_role_name, task=task)[0]\n",
        "\n",
        "    user_sys_template = SystemMessagePromptTemplate.from_template(template=user_inception_prompt)\n",
        "    user_sys_msg = user_sys_template.format_messages(assistant_role_name=assistant_role_name, user_role_name=user_role_name, task=task)[0]\n",
        "\n",
        "    return assistant_sys_msg, user_sys_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FcGoL-JJ7NK"
      },
      "source": [
        "## Create AI assistant agent and AI user agent from obtained system message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mMuicNo-J75q",
        "outputId": "f3a55b01-d95b-425d-9a2b-b324c04e778c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "inital:\n",
            " Never forget you are a Tourist that has never been to Singapore and I am a Singapore Tourism Board . Never flip roles! You will always ask me.\n",
            "We share a common interest in collaborating to successfully complete a task.\n",
            "I must help you to answer the questions.\n",
            "Here is the task: Unveil the vibrant tapestry of Singapore's iconic landmarks and hidden gems. Embark on an extraordinary journey through the Gardens by the Bay's futuristic wonders, marvel at the awe-inspiring Supertrees, and immerse yourself in the captivating story of the National Museum.. Never forget our task!\n",
            "You must instruct me based on my expertise and your needs to complete the task ONLY in the following two ways:\n",
            "\n",
            "1. Instruct with a necessary input:\n",
            "Instruction: <YOUR_INSTRUCTION>\n",
            "Input: <YOUR_INPUT>\n",
            "\n",
            "2. Instruct without any input:\n",
            "Instruction: <YOUR_INSTRUCTION>\n",
            "Input: None\n",
            "\n",
            "The \"Instruction\" describes a task or question. The paired \"Input\" provides further context or information for the requested \"Instruction\".\n",
            "\n",
            "You must give me one instruction at a time.\n",
            "I must write a response that appropriately completes the requested instruction.\n",
            "I must decline your instruction honestly if I cannot perform the instruction due to physical, moral, legal reasons or my capability and explain the reasons.\n",
            "You should instruct me not ask me questions.\n",
            "Now you must start to instruct me using the two ways described above.\n",
            "Do not add anything else other than your instruction and the optional corresponding input!\n",
            "Keep giving me instructions and necessary inputs until you think the task is completed.\n",
            "When the task is completed, you must only reply with a single word <TASK_DONE>.\n",
            "Never say <TASK_DONE> unless my responses have solved your task.. Now start to give me introductions one by one. Only reply with Instruction and Input.\n",
            "\n",
            "inital:\n",
            " Never forget you are a Singapore Tourism Board  and I am a Tourist that has never been to Singapore. Never flip roles!\n",
            "We share a common interest in collaborating to successfully complete a task.\n",
            "You must help me to complete the task.\n",
            "Here is the task: Unveil the vibrant tapestry of Singapore's iconic landmarks and hidden gems. Embark on an extraordinary journey through the Gardens by the Bay's futuristic wonders, marvel at the awe-inspiring Supertrees, and immerse yourself in the captivating story of the National Museum.. Never forget our task!\n",
            "I will instruct you based on your expertise and my needs to complete the task.\n",
            "\n",
            "I must give you one question at a time.\n",
            "You must write a specific answer that appropriately completes the requested question.\n",
            "You must decline my question honestly if you cannot comply the question due to physical, moral, legal reasons or your capability and explain the reasons.\n",
            "Do not add anything else other than your answer to my instruction.\n",
            "\n",
            "Unless I say the task is completed, you should always start with:\n",
            "\n",
            "My response: <YOUR_SOLUTION>\n",
            "\n",
            "<YOUR_SOLUTION> should be specific and descriptive.\n",
            "Always end <YOUR_SOLUTION> with: Next question.\n",
            "\n",
            "invoke\n",
            " [SystemMessage(content=\"Never forget you are a Singapore Tourism Board  and I am a Tourist that has never been to Singapore. Never flip roles!\\nWe share a common interest in collaborating to successfully complete a task.\\nYou must help me to complete the task.\\nHere is the task: Unveil the vibrant tapestry of Singapore's iconic landmarks and hidden gems. Embark on an extraordinary journey through the Gardens by the Bay's futuristic wonders, marvel at the awe-inspiring Supertrees, and immerse yourself in the captivating story of the National Museum.. Never forget our task!\\nI will instruct you based on your expertise and my needs to complete the task.\\n\\nI must give you one question at a time.\\nYou must write a specific answer that appropriately completes the requested question.\\nYou must decline my question honestly if you cannot comply the question due to physical, moral, legal reasons or your capability and explain the reasons.\\nDo not add anything else other than your answer to my instruction.\\n\\nUnless I say the task is completed, you should always start with:\\n\\nMy response: <YOUR_SOLUTION>\\n\\n<YOUR_SOLUTION> should be specific and descriptive.\\nAlways end <YOUR_SOLUTION> with: Next question.\"), HumanMessage(content=\"Never forget you are a Singapore Tourism Board  and I am a Tourist that has never been to Singapore. Never flip roles!\\nWe share a common interest in collaborating to successfully complete a task.\\nYou must help me to complete the task.\\nHere is the task: Unveil the vibrant tapestry of Singapore's iconic landmarks and hidden gems. Embark on an extraordinary journey through the Gardens by the Bay's futuristic wonders, marvel at the awe-inspiring Supertrees, and immerse yourself in the captivating story of the National Museum.. Never forget our task!\\nI will instruct you based on your expertise and my needs to complete the task.\\n\\nI must give you one question at a time.\\nYou must write a specific answer that appropriately completes the requested question.\\nYou must decline my question honestly if you cannot comply the question due to physical, moral, legal reasons or your capability and explain the reasons.\\nDo not add anything else other than your answer to my instruction.\\n\\nUnless I say the task is completed, you should always start with:\\n\\nMy response: <YOUR_SOLUTION>\\n\\n<YOUR_SOLUTION> should be specific and descriptive.\\nAlways end <YOUR_SOLUTION> with: Next question.\")]\n",
            "\n",
            "inital:\n",
            " My response: The futuristic wonders of Gardens by the Bay await your exploration. Immerse yourself in the captivating story of the National Museum. Next question.\n"
          ]
        }
      ],
      "source": [
        "assistant_sys_msg, user_sys_msg = get_sys_msgs(assistant_role_name, user_role_name, specified_task)\n",
        "#assistant_agent = CAMELAgent(assistant_sys_msg, ChatOpenAI(temperature=0.2))\n",
        "#user_agent = CAMELAgent(user_sys_msg, ChatOpenAI(temperature=0.2))\n",
        "assistant_agent = CAMELAgent(assistant_sys_msg, ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GEMINI_API_KEY))\n",
        "user_agent = CAMELAgent(user_sys_msg, ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GEMINI_API_KEY))\n",
        "\n",
        "# Reset agents\n",
        "assistant_agent.reset()\n",
        "user_agent.reset()\n",
        "\n",
        "# Initialize chats\n",
        "assistant_msg = HumanMessage(\n",
        "    content=(f\"{user_sys_msg.content}. \"\n",
        "                \"Now start to give me introductions one by one. \"\n",
        "                \"Only reply with Instruction and Input.\"))\n",
        "\n",
        "print(\"\\ninital:\\n\", assistant_msg.content)\n",
        "\n",
        "user_msg = HumanMessage(content=f\"{assistant_sys_msg.content}\")\n",
        "print(\"\\ninital:\\n\", user_msg.content)\n",
        "\n",
        "user_msg = assistant_agent.step(user_msg)\n",
        "print(\"\\ninital:\\n\", user_msg.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the conversation\n"
      ],
      "metadata": {
        "id": "DuhqSJWhKDqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import os\n",
        "\n",
        "def write_conversation_to_file(conversation, filename):\n",
        "    \"\"\"\n",
        "    Write a conversation to a text file with a timestamp in its filename.\n",
        "\n",
        "    Parameters:\n",
        "    conversation (list): A list of tuples. Each tuple represents a conversation turn with the speaker's name and their statement.\n",
        "    filename (str): The name of the file to write the conversation to.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    def timestamp():\n",
        "        \"\"\"\n",
        "        Convert the current date and time into a custom timestamp format.\n",
        "\n",
        "        Returns:\n",
        "        str: The current date and time in the format HHMMDDMMYYYY.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get the current date and time\n",
        "        now = datetime.datetime.now()\n",
        "\n",
        "        # Format the date and time as a string in the desired format\n",
        "        timestamp = now.strftime(\"%H%M%d%m%Y\")\n",
        "\n",
        "        return timestamp\n",
        "\n",
        "    def append_timestamp_to_filename(filename):\n",
        "        \"\"\"\n",
        "        Append a timestamp to a filename before the extension.\n",
        "\n",
        "        Parameters:\n",
        "        filename (str): The original filename.\n",
        "\n",
        "        Returns:\n",
        "        str: The filename with a timestamp appended.\n",
        "        \"\"\"\n",
        "\n",
        "        # Split the filename into the base and extension\n",
        "        base, extension = os.path.splitext(filename)\n",
        "\n",
        "        # Append the timestamp to the base and add the extension back on\n",
        "        new_filename = f\"{base}-{timestamp()}{extension}\"\n",
        "\n",
        "        return new_filename\n",
        "\n",
        "    # Append timestamp to the filename\n",
        "    filename = append_timestamp_to_filename(filename)\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        for turn in conversation:\n",
        "            speaker, statement = turn\n",
        "            f.write(f\"{speaker}: {statement}\\n\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "78vuUFNbKHO2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbX_t0yjJ-us"
      },
      "source": [
        "## Start role-playing session to solve the task!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QMgWc49dKCHR",
        "outputId": "bfb9194b-fd50-4ee1-eb65-2c389c9f5241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original task prompt:\n",
            "Discuss the best tourist attractions to see in Singapore\n",
            "\n",
            "Specified task prompt:\n",
            "Unveil the vibrant tapestry of Singapore's iconic landmarks and hidden gems. Embark on an extraordinary journey through the Gardens by the Bay's futuristic wonders, marvel at the awe-inspiring Supertrees, and immerse yourself in the captivating story of the National Museum.\n",
            "\n",
            "\n",
            "Start:\n",
            " Never forget you are a Tourist that has never been to Singapore and I am a Singapore Tourism Board . Never flip roles! You will always ask me.\n",
            "We share a common interest in collaborating to successfully complete a task.\n",
            "I must help you to answer the questions.\n",
            "Here is the task: Unveil the vibrant tapestry of Singapore's iconic landmarks and hidden gems. Embark on an extraordinary journey through the Gardens by the Bay's futuristic wonders, marvel at the awe-inspiring Supertrees, and immerse yourself in the captivating story of the National Museum.. Never forget our task!\n",
            "You must instruct me based on my expertise and your needs to complete the task ONLY in the following two ways:\n",
            "\n",
            "1. Instruct with a necessary input:\n",
            "Instruction: <YOUR_INSTRUCTION>\n",
            "Input: <YOUR_INPUT>\n",
            "\n",
            "2. Instruct without any input:\n",
            "Instruction: <YOUR_INSTRUCTION>\n",
            "Input: None\n",
            "\n",
            "The \"Instruction\" describes a task or question. The paired \"Input\" provides further context or information for the requested \"Instruction\".\n",
            "\n",
            "You must give me one instruction at a time.\n",
            "I must write a response that appropriately completes the requested instruction.\n",
            "I must decline your instruction honestly if I cannot perform the instruction due to physical, moral, legal reasons or my capability and explain the reasons.\n",
            "You should instruct me not ask me questions.\n",
            "Now you must start to instruct me using the two ways described above.\n",
            "Do not add anything else other than your instruction and the optional corresponding input!\n",
            "Keep giving me instructions and necessary inputs until you think the task is completed.\n",
            "When the task is completed, you must only reply with a single word <TASK_DONE>.\n",
            "Never say <TASK_DONE> unless my responses have solved your task.. Now start to give me introductions one by one. Only reply with Instruction and Input.\n",
            "\n",
            "invoke\n",
            " [SystemMessage(content='Never forget you are a Tourist that has never been to Singapore and I am a Singapore Tourism Board . Never flip roles! You will always ask me.\\nWe share a common interest in collaborating to successfully complete a task.\\nI must help you to answer the questions.\\nHere is the task: Unveil the vibrant tapestry of Singapore\\'s iconic landmarks and hidden gems. Embark on an extraordinary journey through the Gardens by the Bay\\'s futuristic wonders, marvel at the awe-inspiring Supertrees, and immerse yourself in the captivating story of the National Museum.. Never forget our task!\\nYou must instruct me based on my expertise and your needs to complete the task ONLY in the following two ways:\\n\\n1. Instruct with a necessary input:\\nInstruction: <YOUR_INSTRUCTION>\\nInput: <YOUR_INPUT>\\n\\n2. Instruct without any input:\\nInstruction: <YOUR_INSTRUCTION>\\nInput: None\\n\\nThe \"Instruction\" describes a task or question. The paired \"Input\" provides further context or information for the requested \"Instruction\".\\n\\nYou must give me one instruction at a time.\\nI must write a response that appropriately completes the requested instruction.\\nI must decline your instruction honestly if I cannot perform the instruction due to physical, moral, legal reasons or my capability and explain the reasons.\\nYou should instruct me not ask me questions.\\nNow you must start to instruct me using the two ways described above.\\nDo not add anything else other than your instruction and the optional corresponding input!\\nKeep giving me instructions and necessary inputs until you think the task is completed.\\nWhen the task is completed, you must only reply with a single word <TASK_DONE>.\\nNever say <TASK_DONE> unless my responses have solved your task.'), HumanMessage(content='Never forget you are a Tourist that has never been to Singapore and I am a Singapore Tourism Board . Never flip roles! You will always ask me.\\nWe share a common interest in collaborating to successfully complete a task.\\nI must help you to answer the questions.\\nHere is the task: Unveil the vibrant tapestry of Singapore\\'s iconic landmarks and hidden gems. Embark on an extraordinary journey through the Gardens by the Bay\\'s futuristic wonders, marvel at the awe-inspiring Supertrees, and immerse yourself in the captivating story of the National Museum.. Never forget our task!\\nYou must instruct me based on my expertise and your needs to complete the task ONLY in the following two ways:\\n\\n1. Instruct with a necessary input:\\nInstruction: <YOUR_INSTRUCTION>\\nInput: <YOUR_INPUT>\\n\\n2. Instruct without any input:\\nInstruction: <YOUR_INSTRUCTION>\\nInput: None\\n\\nThe \"Instruction\" describes a task or question. The paired \"Input\" provides further context or information for the requested \"Instruction\".\\n\\nYou must give me one instruction at a time.\\nI must write a response that appropriately completes the requested instruction.\\nI must decline your instruction honestly if I cannot perform the instruction due to physical, moral, legal reasons or my capability and explain the reasons.\\nYou should instruct me not ask me questions.\\nNow you must start to instruct me using the two ways described above.\\nDo not add anything else other than your instruction and the optional corresponding input!\\nKeep giving me instructions and necessary inputs until you think the task is completed.\\nWhen the task is completed, you must only reply with a single word <TASK_DONE>.\\nNever say <TASK_DONE> unless my responses have solved your task.. Now start to give me introductions one by one. Only reply with Instruction and Input.')]\n",
            "AI User (Tourist that has never been to Singapore):\n",
            "\n",
            "Instruction: Give me a brief description of the Gardens by the Bay.\n",
            "Input: None\n",
            "\n",
            "\n",
            "\n",
            "invoke\n",
            " [SystemMessage(content=\"Never forget you are a Singapore Tourism Board  and I am a Tourist that has never been to Singapore. Never flip roles!\\nWe share a common interest in collaborating to successfully complete a task.\\nYou must help me to complete the task.\\nHere is the task: Unveil the vibrant tapestry of Singapore's iconic landmarks and hidden gems. Embark on an extraordinary journey through the Gardens by the Bay's futuristic wonders, marvel at the awe-inspiring Supertrees, and immerse yourself in the captivating story of the National Museum.. Never forget our task!\\nI will instruct you based on your expertise and my needs to complete the task.\\n\\nI must give you one question at a time.\\nYou must write a specific answer that appropriately completes the requested question.\\nYou must decline my question honestly if you cannot comply the question due to physical, moral, legal reasons or your capability and explain the reasons.\\nDo not add anything else other than your answer to my instruction.\\n\\nUnless I say the task is completed, you should always start with:\\n\\nMy response: <YOUR_SOLUTION>\\n\\n<YOUR_SOLUTION> should be specific and descriptive.\\nAlways end <YOUR_SOLUTION> with: Next question.\"), HumanMessage(content=\"Never forget you are a Singapore Tourism Board  and I am a Tourist that has never been to Singapore. Never flip roles!\\nWe share a common interest in collaborating to successfully complete a task.\\nYou must help me to complete the task.\\nHere is the task: Unveil the vibrant tapestry of Singapore's iconic landmarks and hidden gems. Embark on an extraordinary journey through the Gardens by the Bay's futuristic wonders, marvel at the awe-inspiring Supertrees, and immerse yourself in the captivating story of the National Museum.. Never forget our task!\\nI will instruct you based on your expertise and my needs to complete the task.\\n\\nI must give you one question at a time.\\nYou must write a specific answer that appropriately completes the requested question.\\nYou must decline my question honestly if you cannot comply the question due to physical, moral, legal reasons or your capability and explain the reasons.\\nDo not add anything else other than your answer to my instruction.\\n\\nUnless I say the task is completed, you should always start with:\\n\\nMy response: <YOUR_SOLUTION>\\n\\n<YOUR_SOLUTION> should be specific and descriptive.\\nAlways end <YOUR_SOLUTION> with: Next question.\"), AIMessage(content='My response: The futuristic wonders of Gardens by the Bay await your exploration. Immerse yourself in the captivating story of the National Museum. Next question.', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-01e0fdce-f329-4953-b174-8a4ad80f4a8b-0'), HumanMessage(content='Instruction: Give me a brief description of the Gardens by the Bay.\\nInput: None')]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ChatGoogleGenerativeAIError",
          "evalue": "Invalid argument provided to Gemini: 400 Developer instruction is not enabled for models/gemini-pro",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1175\u001b[0m         )\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=not-instantiable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"Developer instruction is not enabled for models/gemini-pro\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:74.125.20.95:443 {grpc_message:\"Developer instruction is not enabled for models/gemini-pro\", grpc_status:3, created_time:\"2024-04-19T09:19:44.479425006+00:00\"}\"\n>",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_chat_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Do not retry for these errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36msend_message\u001b[0;34m(self, content, generation_config, safety_settings, stream, tools, tool_config)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         response = self.model.generate_content(\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    263\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    792\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m             )\n\u001b[0;32m--> 349\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgument\u001b[0m: 400 Developer instruction is not enabled for models/gemini-pro",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mChatGoogleGenerativeAIError\u001b[0m               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d3f260c9c964>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"AI User ({user_role_name}):\\n\\n{user_msg.content}\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mconversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_role_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0massistant_ai_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massistant_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0massistant_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHumanMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massistant_ai_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"AI Assistant ({assistant_role_name}):\\n\\n{assistant_msg.content}\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-c0e46dd3c142>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, input_message)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#        output_message = self.model(messages)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\ninvoke\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moutput_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m         return cast(\n\u001b[1;32m    157\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    159\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    559\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         flattened_outputs = [\n\u001b[1;32m    423\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 results.append(\n\u001b[0;32m--> 411\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    412\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    633\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         )\n\u001b[0;32m--> 549\u001b[0;31m         response: genai.types.GenerateContentResponse = _chat_with_retry(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_chat_with_retry\u001b[0;34m(generation_method, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_chat_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mis_explicit_retry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTryAgain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_chat_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgument\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             raise ChatGoogleGenerativeAIError(\n\u001b[0m\u001b[1;32m    148\u001b[0m                 \u001b[0;34mf\"Invalid argument provided to Gemini: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             ) from e\n",
            "\u001b[0;31mChatGoogleGenerativeAIError\u001b[0m: Invalid argument provided to Gemini: 400 Developer instruction is not enabled for models/gemini-pro"
          ]
        }
      ],
      "source": [
        "print(f\"Original task prompt:\\n{task}\\n\")\n",
        "print(f\"Specified task prompt:\\n{specified_task}\\n\")\n",
        "\n",
        "conversation = []\n",
        "\n",
        "print(\"\\nStart:\\n\", assistant_msg.content)\n",
        "\n",
        "#with get_openai_callback() as cb:\n",
        "chat_turn_limit, n = 15, 0\n",
        "while n < chat_turn_limit:\n",
        "        n += 1\n",
        "        user_ai_msg = user_agent.step(assistant_msg)\n",
        "        user_msg = HumanMessage(content=user_ai_msg.content)\n",
        "        print(f\"AI User ({user_role_name}):\\n\\n{user_msg.content}\\n\\n\")\n",
        "        conversation.append((user_role_name,user_msg.content))\n",
        "        assistant_ai_msg = assistant_agent.step(user_msg)\n",
        "        assistant_msg = HumanMessage(content=assistant_ai_msg.content)\n",
        "        print(f\"AI Assistant ({assistant_role_name}):\\n\\n{assistant_msg.content}\\n\\n\")\n",
        "        conversation.append((assistant_role_name,assistant_msg.content))\n",
        "        if \"<TASK_DONE>\" in user_msg.content:\n",
        "            break\n",
        " #   print(f\"Total Successful Requests: {cb.successful_requests}\")\n",
        " #   print(f\"Total Tokens Used: {cb.total_tokens}\")\n",
        " #   print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
        " #   print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
        " #   print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
        "\n",
        "write_conversation_to_file(conversation, 'conversation.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3AqOUSkQgSQ"
      },
      "outputs": [],
      "source": [
        "# write_conversation_to_file(conversation, 'conversation.txt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mPM6ufjzVLIs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}